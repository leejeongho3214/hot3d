{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee215408-e60b-4209-a09a-f84a8fd5ffa1",
   "metadata": {},
   "source": [
    "\n",
    "# Hot3D Data Provider Tutorial\n",
    "\n",
    "In order to use sequences from the HOT3D dataset, you will need ot use the Hot3dDataProvider object.\n",
    "\n",
    "This notebook is explaining how to use the various \"DataProvider\" in order to retrieve:\n",
    "- Section 0: DataProvider initialization\n",
    "- Section 1: Device calibration and Image data\n",
    "- Section 2: Pose data\n",
    "  - Section 2.a: Device/Headset pose data\n",
    "  - Section 2.b: Hand pose data\n",
    "  - Section 2.b.a: Hand pose data and MESH hands\n",
    "  - Section 2.c: Object pose data\n",
    "- Section 3:\n",
    "  - Section 3.a: Object bounding boxes (amodal bounding boxes)\n",
    "  - Section 3.b: Hand bounding boxes (amodal bounding boxes)\n",
    "- Section 4: Eye Gaze data (only for Aria data)\n",
    "- Section 5: Camera reprojection (reprojection hand vertices to raw fish images)\n",
    "\n",
    "Hot3dDataProvider API is organized as follow:\n",
    "```\n",
    "|- device_data_provider        -> provides device calibration and image data\n",
    "|- device_pose_data_provider   -> provides device pose data\n",
    "|- mano_hand_data_provider     -> provides hand pose data (MANO representation)\n",
    "|- umetrack_hand_data_provider -> provides hand pose data (UmeTrack representation)\n",
    "|- object_pose_data_provider   -> provides object pose data\n",
    "|- object_library              -> provides information about the HOT3D 3D objects/assets\n",
    "|- hand_box2d_data_provider    -> provides hands bbox information\n",
    "|- object_box2d_data_provider  -> provides objects bbox information\n",
    "```\n",
    "\n",
    "## Notes\n",
    "- All Device/Headset, Hand, Object poses data are shared in world coordinates (meters)\n",
    "\n",
    "In this tutorial you will learn that:\n",
    "- Device data, such as Image data stream is indexed with a stream_id\n",
    "- Headset use camera rig coordinates relative to the DEVICE pose (world_camera_stream_id = world_device @ device_camera_stream_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bcb96c-4910-4a60-9a41-c3b34ee7ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a MANO model, with only 10 shape coefficients.\n",
      "num_betas=10, shapedirs.shape=(778, 3, 10), self.SHAPE_SPACE_DIM=300\n",
      "WARNING: You are using a MANO model, with only 10 shape coefficients.\n",
      "num_betas=10, shapedirs.shape=(778, 3, 10), self.SHAPE_SPACE_DIM=300\n",
      "MPS Data Paths\n",
      "MPS SLAM Data Paths\n",
      "--closedLoopTrajectory: /home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/slam/closed_loop_trajectory.csv\n",
      "--openLoopTrajectory: /home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/slam/open_loop_trajectory.csv\n",
      "--semidensePoints: /home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/slam/semidense_points.csv.gz\n",
      "--semidenseObservations: /home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/slam/semidense_observations.csv.gz\n",
      "--onlineCalibration: /home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/slam/online_calibration.jsonl\n",
      "--summary: /home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/slam/summary.json\n",
      "MPS Eyegaze Data Paths\n",
      "--generalEyegaze: /home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/eye_gaze/general_eye_gaze.csv\n",
      "--personalizedEyegaze: /home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/eye_gaze/personalized_eye_gaze.csv\n",
      "--summary: /home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/eye_gaze/summary.json\n",
      "MPS Hand Tracking Data Paths\n",
      "--wristAndPalmPoses: \n",
      "--summary: \n",
      "data_provider statistics: {'dynamic_objects': {'num_frames': 7374, 'num_objects': 6, 'object_uids': ['171735388712249', '4111539686391', '204462113746498', '125863066770940', '183136364331389', '143541090750632']}, 'mano_hand_poses': {'num_frames': 7374, 'num_right_hands': 7320, 'num_left_hands': 7374}, 'umetrack_hand_poses': {'num_frames': 7374, 'num_right_hands': 7320, 'num_left_hands': 7374}, 'object_box2ds': {'num_frames': {'1201-1': 3687, '1201-2': 3687, '214-1': 3687}, 'stream_ids': ['1201-1', '1201-2', '214-1'], 'num_objects': 6, 'object_uids': ['4111539686391', '171735388712249', '204462113746498', '143541090750632', '125863066770940', '183136364331389']}, 'hand_box2ds': {'num_frames': {'214-1': 3687, '1201-1': 3687, '1201-2': 3687}, 'stream_ids': ['214-1', '1201-1', '1201-2']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m[MultiRecordFileReader][DEBUG]: Opened file '/home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/recording.vrs' and assigned to reader #0\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 214-1/camera-rgb activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;255;165;000m[VrsDataProvider][WARNING]: Unsupported TimeSync mode: APP, ignoring.\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: Timecode stream found: 285-2\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: Fail to activate streamId 286-1\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-1/camera-slam-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-2/camera-slam-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-1/imu-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-2/imu-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;255;165;000m[MpsDataPathsProvider][WARNING]: Hand tracking folder (/home/jeongho/Dataset/ljh/dataset/hot3d/full-hot3d/P0003_5766eae8/mps/hand_tracking) does not exist in MPS root folder, not loading wrist and palm poses.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Section 0: DataProvider initialization\n",
    "#\n",
    "# Take home message:\n",
    "# - Device data, such as Image data stream is indexed with a stream_id\n",
    "# - Intrinsics and Extrinsics calibration relative to the device coordinates is available for each CAMERA/stream_id\n",
    "#\n",
    "# Data Requirements:\n",
    "# - a sequence\n",
    "# - the object library\n",
    "# Optional:\n",
    "# - To use the Mano hand you need to have the LEFT/RIGHT *.pkl hand models (available)\n",
    "\n",
    "import os\n",
    "from dataset_api import Hot3dDataProvider\n",
    "from data_loaders.loader_object_library import load_object_library\n",
    "from data_loaders.mano_layer import MANOHandModel\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "hot3d_dataset_path = home + \"/Dataset/ljh/dataset/hot3d/full-hot3d\"\n",
    "sequence_path = os.path.join(hot3d_dataset_path, \"P0003_5766eae8\")\n",
    "object_library_path = home +\"/Dataset/ljh/dataset/hot3d/assets\"\n",
    "mano_hand_model_path = home + \"/dir/mano_v1_2/models\"\n",
    "\n",
    "if not os.path.exists(sequence_path) or not os.path.exists(object_library_path):\n",
    "    print(\"Invalid input sequence or library path.\")\n",
    "    print(\"Please do update the path to VALID values for your system.\")\n",
    "    raise\n",
    "#\n",
    "# Init the object library\n",
    "#\n",
    "object_library = load_object_library(object_library_folderpath=object_library_path)\n",
    "\n",
    "#\n",
    "# Init the HANDs model\n",
    "# If None, the UmeTrack HANDs model will be used\n",
    "#\n",
    "mano_hand_model = None\n",
    "if mano_hand_model_path is not None:\n",
    "    mano_hand_model = MANOHandModel(mano_hand_model_path)\n",
    "\n",
    "#\n",
    "# Initialize hot3d data provider\n",
    "#\n",
    "hot3d_data_provider = Hot3dDataProvider(\n",
    "    sequence_folder=sequence_path,\n",
    "    object_library=object_library,\n",
    "    mano_hand_model=mano_hand_model,\n",
    ")\n",
    "print(f\"data_provider statistics: {hot3d_data_provider.get_data_statistics()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24dae021-1cfe-440a-a5c8-6f44851219e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# Used for interactive display in the following sections\n",
    "#\n",
    "#\n",
    "import rerun as rr\n",
    "import numpy as np\n",
    "\n",
    "from projectaria_tools.core.sophus import SE3\n",
    "from projectaria_tools.utils.rerun_helpers import ToTransform3D\n",
    "\n",
    "\n",
    "def log_image(\n",
    "    image: np.array,\n",
    "    label: str,\n",
    "    static=False\n",
    ") -> None:\n",
    "    rr.log(label, rr.Image(image), static=static)\n",
    "\n",
    "\n",
    "def log_pose(\n",
    "    pose: SE3,\n",
    "    label: str,\n",
    "    static=False\n",
    ") -> None:\n",
    "    rr.log(label, ToTransform3D(pose, False), static=static)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d199e9c-7359-43b2-8ecd-807db548def0",
   "metadata": {},
   "source": [
    "# A gentle introduction to the \"GT Data\" Provider API\n",
    "\n",
    "Take home message:\n",
    "- All \"GT data provider\" are using a similar API interface to query data at a given timestamp and/or StreamID.\n",
    "- If the requested timestamp does not exists, the closest one can be retrieve along its delta time (dt).\n",
    "\n",
    "All the following \"GT data providers\" are accessible from Hot3dDataProvider and using a similar API interface.\n",
    "```\n",
    "|- device_pose_data_provider   -> device/headset pose data\n",
    "|- mano_hand_data_provider     -> hand pose data (MANO hand model)\n",
    "|- umetrack_hand_data_provider -> hand pose data (UmeTrack hand model)\n",
    "|- object_pose_data_provider   -> object pose data\n",
    "|- hand_box2d_data_provider    -> hand information such as amodal BBox and visibility ratio\n",
    "|- object_box2d_data_provider  -> object information such as amodal BBox and visibility ratio\n",
    "```\n",
    "\n",
    "We are here shortly introducing the retrieval concept used, and then will showcase how to use each data_provider.\n",
    "GT data providers enable retrieving information at a given TIMESTAMP\n",
    "- If the timestamp is not exact, the closest one can will be returned,\n",
    "- Delta Time (dt) between the found sample and the query timestamp is returned\n",
    "  Meaning that you known if you have a perfect match to the GT time sample or retrieved a close sample.\n",
    "  \n",
    "Note: Some GT data providers are STREAM_ID specific and enable retrieve information for a given image stream.\n",
    "```\n",
    "data_with_dt = device_pose_provider.get_pose_at_timestamp(\n",
    "   timestamp_ns: int,                           -> Timestamp\n",
    "   stream_id: StreamID,                         -> If used, specify for which VRS image stream you query the data\n",
    "   time_query_options: TimeQueryOptions,        -> Retrieval configuration, i.e TimeQueryOptions.CLOSEST\n",
    "   time_domain: TimeDomain,                     -> TimeDomain (always use TimeDomain.TIME_CODE)\n",
    "   acceptable_time_delta: Optional[int] = None, -> Threshold to reject delta dt that would be too large (using 0 or None is recommended)\n",
    "```\n",
    "\n",
    "Here is how most of the interface will be used in the following sections:\n",
    "```\n",
    "data_with_dt = X_provider.get_X_at_timestamp(\n",
    "    timestamp_ns=timestamp_ns,\n",
    "    time_query_options=TimeQueryOptions.CLOSEST,\n",
    "    time_domain=TimeDomain.TIME_CODE)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12767376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-06T22:08:56Z WARN  re_sdk::log_sink] Dropping data in MemorySink\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: P0003_5766eae8\n",
      "Device type is Headset.Aria\n",
      "Image stream ids: [214-1, 1201-1, 1201-2]\n",
      "Number of timestamp for this sequence: 3687\n",
      "Duration of the sequence: 122.866632014 (seconds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 50.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CameraCalibration(label: camera-rgb, model name: Fisheye624, principal point: [707.397, 707.154], focal length: [609.195, 609.195], projection params: [609.195, 707.397, 707.154, 0.389402, -0.387784, -0.125112, 1.55909, -1.98291, 0.720775, -0.000469152, 0.000364496, 0.000965818, -0.000290893, -0.00111536, -0.000101303], image size (w,h): [1408, 1408], T_Device_Camera:(translation:[-0.00414696, -0.0122964, -0.00474385], quaternion(x,y,z,w):[0.327852, 0.0378477, 0.0316434, 0.94344]), serialNumber:0450577b730410834401100000000000)\n",
      "CameraCalibration(label: camera-slam-left, model name: Fisheye624, principal point: [318.229, 236.975], focal length: [241.414, 241.414], projection params: [241.414, 318.229, 236.975, -0.0283439, 0.104745, -0.0759371, 0.0158346, -0.000111902, -0.000199961, 0.000929693, -0.00184501, -0.000276403, -7.76735e-05, 0.00234218, -6.94112e-05], image size (w,h): [640, 480], T_Device_Camera:(translation:[1.56125e-17, -1.38778e-17, 3.46945e-18], quaternion(x,y,z,w):[0, 0, 0, 1]), serialNumber:0072510f1b0c0701070000080c0a0001)\n",
      "CameraCalibration(label: camera-slam-right, model name: Fisheye624, principal point: [321.211, 237.815], focal length: [241.145, 241.145], projection params: [241.145, 321.211, 237.815, -0.0243262, 0.0935968, -0.0608284, 0.00531246, 0.00356175, -0.000709423, 0.00315881, -0.0017897, -0.0030157, 0.00012122, 0.00303343, 0.000140858], image size (w,h): [640, 480], T_Device_Camera:(translation:[0.00459607, -0.10884, -0.0845098], quaternion(x,y,z,w):[0.613318, -0.000557023, 0.0282732, 0.78933]), serialNumber:0072510f1b0c0701070000082e100001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Device calibration and Image data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "# Retrieve some statistics about the \"IMAGE\" VRS recording\n",
    "#\n",
    "\n",
    "# Getting the device data provider (alias)\n",
    "device_data_provider = hot3d_data_provider.device_data_provider\n",
    "\n",
    "# Retrieve the list of image stream supported by this sequence\n",
    "# It will return the RGB and SLAM Left/Right image streams\n",
    "image_stream_ids = device_data_provider.get_image_stream_ids()\n",
    "# Retrieve a list of timestamps for the sequence (in nanoseconds)\n",
    "timestamps = device_data_provider.get_sequence_timestamps()\n",
    "\n",
    "print(f\"Sequence: {os.path.basename(os.path.normpath(sequence_path))}\")\n",
    "print(f\"Device type is {hot3d_data_provider.get_device_type()}\")\n",
    "print(f\"Image stream ids: {image_stream_ids}\")\n",
    "print(f\"Number of timestamp for this sequence: {len(timestamps)}\")\n",
    "print(\n",
    "    f\"Duration of the sequence: {(timestamps[-1] - timestamps[0]) / 1e9} (seconds)\"\n",
    ")  # Timestamps are in nanoseconds\n",
    "\n",
    "\n",
    "# Init a rerun context to visualize the sequence file images\n",
    "rr.init(\"Device images\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# How to iterate over timestamps using a slice to show one timestamp every 200\n",
    "timestamps_slice = slice(None, None, 200)\n",
    "# Loop over the timestamps of the sequence and visualize corresponding data\n",
    "for timestamp_ns in tqdm(timestamps[timestamps_slice]):\n",
    "\n",
    "    for stream_id in image_stream_ids:\n",
    "        # Retrieve the image stream label as string\n",
    "        image_stream_label = device_data_provider.get_image_stream_label(stream_id)\n",
    "        # Retrieve the image data for a given timestamp\n",
    "        image_data = device_data_provider.get_image(timestamp_ns, stream_id)\n",
    "        # Visualize the image data (it's a numpy array)\n",
    "        log_image(label=f\"img/{image_stream_label}\", image=image_data)\n",
    "\n",
    "\n",
    "#\n",
    "# Retrieve Camera calibration (intrinsics and extrinsics) for a given stream_id\n",
    "#\n",
    "for stream_id in image_stream_ids:\n",
    "    # Retrieve the camera calibration (intrinsics and extrinsics) for a given stream_id\n",
    "    [extrinsics, intrinsics] = device_data_provider.get_camera_calibration(stream_id)\n",
    "    print(intrinsics)\n",
    "    # We will show in next section how to visualize the position of the camera in the world frame\n",
    "\n",
    "# Showing the rerun window\n",
    "# rr.notebook_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c473c3-174f-4cff-9d58-0650b580da72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m pose_translations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Retrieve the position of the device in the world frame at a given timestamp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestamp_ns \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(timestamps):\n\u001b[1;32m     23\u001b[0m     rr\u001b[38;5;241m.\u001b[39mset_time_nanos(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynchronization_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m(timestamp_ns))\n\u001b[1;32m     24\u001b[0m     rr\u001b[38;5;241m.\u001b[39mset_time_sequence(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, timestamp_ns)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Section 2: Pose data\n",
    "#\n",
    "# Take home message:\n",
    "# - the device_pose_provider enables you to retrieve the Headset pose as (T_world_device)\n",
    "# - amoving to the device to a given camera can be done by using calibration data and combining SE3 poses\n",
    "#   - such as T_world_camera = T_world_device @ T_device_camera\n",
    "#\n",
    "\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "\n",
    "# Alias over the HEADSET/Device pose data provider\n",
    "device_pose_provider = hot3d_data_provider.device_pose_data_provider\n",
    "\n",
    "# Init a rerun context to visualize the device trajectory\n",
    "rr.init(\"Device/Headset trajectory\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "pose_translations = []\n",
    "# Retrieve the position of the device in the world frame at a given timestamp\n",
    "for timestamp_ns in tqdm(timestamps):\n",
    "\n",
    "    rr.set_time_nanos(\"synchronization_time\", int(timestamp_ns))\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp_ns)\n",
    "\n",
    "    headset_pose3d_with_dt = None\n",
    "    if device_pose_provider is None:\n",
    "        continue\n",
    "    headset_pose3d_with_dt = device_pose_provider.get_pose_at_timestamp(\n",
    "        timestamp_ns=timestamp_ns,\n",
    "        time_query_options=TimeQueryOptions.CLOSEST,\n",
    "        time_domain=TimeDomain.TIME_CODE,\n",
    "    )\n",
    "\n",
    "    if headset_pose3d_with_dt is None:\n",
    "        continue\n",
    "\n",
    "    headset_pose3d = headset_pose3d_with_dt.pose3d\n",
    "    T_world_device = headset_pose3d.T_world_device\n",
    "    \n",
    "    log_pose(pose=T_world_device, label=\"world/device\")\n",
    "    pose_translations.append(T_world_device.translation()[0])\n",
    "    # This is the pose of the device, to move to a given camera, you need to apply the device_camera transformation\n",
    "    #for stream_id in image_stream_ids:\n",
    "       # # Retrieve the camera calibration (intrinsics and extrinsics) for a given stream_id\n",
    "       # [T_device_camera, intrinsics] = device_data_provider.get_camera_calibration(stream_id)\n",
    "       # # The pose of the given camera at this timestamp is (world_camera = world_device @ device_camera):\n",
    "       # T_world_camera = headset_pose3d.T_world_device @ T_device_camera\n",
    "       # camera_stream_label = device_data_provider.get_image_stream_label(stream_id)\n",
    "       # print(f\"Image stream label: {camera_stream_label} -> world_camera translation: {T_world_camera.translation()[0]}\")\n",
    "\n",
    "rr.log(\"world/device_trajectory\", rr.LineStrips3D([pose_translations]), static=True)\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5253618-2ca2-4d11-a605-aa5aa19c54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Section 2.b: Hand pose data\n",
    "#\n",
    "# Take home message:\n",
    "# - Hands are labelled as LEFT or RIGHT hands\n",
    "# - \"Hands pose\" are representing the WRIST pose on which a MESH or LANDMARKS can be attached (see next section)\n",
    "#\n",
    "\n",
    "# Alias over the HAND pose data provider\n",
    "hand_data_provider = hot3d_data_provider.mano_hand_data_provider if hot3d_data_provider.mano_hand_data_provider is not None else hot3d_data_provider.umetrack_hand_data_provider\n",
    "\n",
    "# Init a rerun context to visualize the hand pose data trajectory\n",
    "rr.init(\"Hand pose trajectory (wrist)\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# Accumulate HAND poses translations as list, to show a LINE strip HAND trajectory\n",
    "left_hand_pose_translations = []\n",
    "right_hand_pose_translations = []\n",
    "\n",
    "# Retrieve the position of the device in the world frame at a given timestamp\n",
    "for timestamp_ns in tqdm(timestamps):\n",
    "\n",
    "    rr.set_time_nanos(\"synchronization_time\", int(timestamp_ns))\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp_ns)\n",
    "\n",
    "    hand_poses_with_dt = None\n",
    "    if hand_data_provider is None:\n",
    "        continue\n",
    "    \n",
    "    hand_poses_with_dt = hand_data_provider.get_pose_at_timestamp(\n",
    "        timestamp_ns=timestamp_ns,\n",
    "        time_query_options=TimeQueryOptions.CLOSEST,\n",
    "        time_domain=TimeDomain.TIME_CODE,\n",
    "    )\n",
    "\n",
    "    if hand_poses_with_dt is None:\n",
    "        continue\n",
    "        \n",
    "    hand_pose_collection = hand_poses_with_dt.pose3d_collection\n",
    "\n",
    "    for hand_pose_data in hand_pose_collection.poses.values():\n",
    "        # Retrieve the handedness of the hand (i.e Left or Right)\n",
    "        handedness_label = hand_pose_data.handedness_label()\n",
    "\n",
    "        T_world_wrist = hand_pose_data.wrist_pose\n",
    "        log_pose(pose=T_world_wrist, label=f\"world/hand/{handedness_label}\")\n",
    "\n",
    "        # Accumulate HAND poses translations as list, to show a LINE strip HAND trajectory\n",
    "        if hand_pose_data.is_left_hand():\n",
    "            left_hand_pose_translations.append(T_world_wrist.translation()[0])\n",
    "        elif hand_pose_data.is_right_hand():\n",
    "            right_hand_pose_translations.append(T_world_wrist.translation()[0])\n",
    "\n",
    "rr.log(\"world/left_hand\", rr.LineStrips3D([left_hand_pose_translations]), static=True)\n",
    "rr.log(\"world/right_hand\", rr.LineStrips3D([right_hand_pose_translations]), static=True)\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a013c63-65e6-4c38-b378-141c8c7ae3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Section 2.b.a: Hand pose data\n",
    "#\n",
    "# Take home message:\n",
    "# - Hands are labelled as LEFT or RIGHT hands\n",
    "# - Hands can be retrieved as:\n",
    "#   - Landmarks and displayed as line\n",
    "#   - Vertices\n",
    "#   - Mesh (using vertices, faces index and normals)\n",
    "#\n",
    "\n",
    "from data_loaders.hand_common import LANDMARK_CONNECTIVITY\n",
    "\n",
    "\n",
    "# Alias over the HAND pose data provider\n",
    "hand_data_provider = hot3d_data_provider.mano_hand_data_provider if hot3d_data_provider.mano_hand_data_provider is not None else hot3d_data_provider.umetrack_hand_data_provider\n",
    "\n",
    "# Init a rerun context\n",
    "rr.init(\"Hand pose LANDMARK/MESH\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "left_hand_pose_translations = []\n",
    "right_hand_pose_translations = []\n",
    "\n",
    "# Limit to the first 300 timestamps\n",
    "for timestamp_ns in tqdm(timestamps[:300]):\n",
    "\n",
    "    rr.set_time_nanos(\"synchronization_time\", int(timestamp_ns))\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp_ns)\n",
    "\n",
    "    hand_poses_with_dt = None\n",
    "    if hand_data_provider is None:\n",
    "        continue\n",
    "        \n",
    "    hand_poses_with_dt = hand_data_provider.get_pose_at_timestamp(\n",
    "        timestamp_ns=timestamp_ns,\n",
    "        time_query_options=TimeQueryOptions.CLOSEST,\n",
    "        time_domain=TimeDomain.TIME_CODE,\n",
    "    )\n",
    "\n",
    "    if hand_poses_with_dt is None:\n",
    "        continue\n",
    "    \n",
    "    hand_pose_collection = hand_poses_with_dt.pose3d_collection\n",
    "\n",
    "    for hand_pose_data in hand_pose_collection.poses.values():\n",
    "        # Retrieve the handedness of the hand (i.e Left or Right)\n",
    "        handedness_label = hand_pose_data.handedness_label()\n",
    "\n",
    "        # Skeleton/Joints landmark representation (for LEFT hand)\n",
    "        if hand_pose_data.is_left_hand():\n",
    "            hand_landmarks = hand_data_provider.get_hand_landmarks(\n",
    "                hand_pose_data\n",
    "            )\n",
    "            # convert landmarks to connected lines for display\n",
    "            # (i.e retrieve points along the HAND LANDMARK_CONNECTIVITY as a list)\n",
    "            points = [connections\n",
    "                      for connectivity in LANDMARK_CONNECTIVITY\n",
    "                      for connections in [[hand_landmarks[it].numpy().tolist() for it in connectivity]]]\n",
    "            rr.log(\n",
    "                f\"world/{handedness_label}/joints\",\n",
    "                rr.LineStrips3D(points, radii=0.002),\n",
    "            )\n",
    "\n",
    "        #\n",
    "        # Plot RIGHT hand as a Triangular Mesh representation\n",
    "        #\n",
    "        if hand_pose_data.is_right_hand():\n",
    "            hand_mesh_vertices = hand_data_provider.get_hand_mesh_vertices(hand_pose_data)\n",
    "            hand_triangles, hand_vertex_normals = hand_data_provider.get_hand_mesh_faces_and_normals(hand_pose_data)\n",
    "            \n",
    "            rr.log(\n",
    "                f\"world/{handedness_label}/mesh_faces\",\n",
    "                rr.Mesh3D(\n",
    "                    vertex_positions=hand_mesh_vertices,\n",
    "                    vertex_normals=hand_vertex_normals,\n",
    "                    triangle_indices=hand_triangles,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909427f-d8c5-40a7-8eba-8702de2a313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Section 2.c: Object pose data\n",
    "#\n",
    "# Take home message:\n",
    "# - Each object is associated with a Unique Identified (uid)\n",
    "# - The object library enables to retrieve the 3D asset linked to this UID (a glb file)\n",
    "#\n",
    "\n",
    "from data_loaders.loader_object_library import ObjectLibrary\n",
    "\n",
    "# Alias over the Object pose data provider\n",
    "object_pose_data_provider = hot3d_data_provider.object_pose_data_provider\n",
    "\n",
    "# Keep track of what 3D assets has been loaded/unloaded so we will load them only when needed\n",
    "# So we will load them only when required for Rerun\n",
    "object_cache_status = {}\n",
    "\n",
    "# Init a rerun context\n",
    "rr.init(\"Object pose\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# Limit to the some timestamps\n",
    "for timestamp_ns in tqdm(timestamps[100:300]):\n",
    "\n",
    "    rr.set_time_nanos(\"synchronization_time\", int(timestamp_ns))\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp_ns)\n",
    "\n",
    "    object_poses_with_dt = (\n",
    "        object_pose_data_provider.get_pose_at_timestamp(\n",
    "            timestamp_ns=timestamp_ns,\n",
    "            time_query_options=TimeQueryOptions.CLOSEST,\n",
    "            time_domain=TimeDomain.TIME_CODE,\n",
    "        )\n",
    "    )\n",
    "    if object_poses_with_dt is None:\n",
    "        continue\n",
    "\n",
    "    objects_pose3d_collection = object_poses_with_dt.pose3d_collection\n",
    "\n",
    "    # Keep a mapping to know what object has been seen, and which one has not\n",
    "    object_uids = object_pose_data_provider.object_uids_with_poses\n",
    "    logging_status = {x: False for x in object_uids}\n",
    "\n",
    "    for (\n",
    "        object_uid,\n",
    "        object_pose3d,\n",
    "    ) in objects_pose3d_collection.poses.items():\n",
    "\n",
    "        object_name = object_library.object_id_to_name_dict[object_uid]\n",
    "        object_name = object_name + \"_\" + str(object_uid)\n",
    "        object_cad_asset_filepath = ObjectLibrary.get_cad_asset_path(\n",
    "            object_library_folderpath=object_library.asset_folder_name,\n",
    "            object_id=object_uid,\n",
    "        )\n",
    "\n",
    "        log_pose(pose=object_pose3d.T_world_object, label=f\"world/objects/{object_name}\")\n",
    "        \n",
    "        # Mark object has been seen (enable to know which object has been logged or not)\n",
    "        # I.E and object not logged, has not been seen and will have its entity cleared for rerun\n",
    "        logging_status[object_uid] = True\n",
    "\n",
    "        # Link the corresponding 3D object to the pose\n",
    "        if object_uid not in object_cache_status.keys():\n",
    "            object_cache_status[object_uid] = True\n",
    "            rr.log(\n",
    "                f\"world/objects/{object_name}\",\n",
    "                rr.Asset3D(\n",
    "                    path=object_cad_asset_filepath,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    # Rerun specifics (if an entity is disapearing, the last status is shown)\n",
    "    # To compensate that , if some objects are not visible, we clear the entity\n",
    "    for object_uid, displayed in logging_status.items():\n",
    "        if not displayed:\n",
    "            object_name = object_library.object_id_to_name_dict[object_uid]\n",
    "            object_name = object_name + \"_\" + str(object_uid)\n",
    "            rr.log(\n",
    "                f\"world/objects/{object_name}\",\n",
    "                rr.Clear.recursive(),\n",
    "            )\n",
    "            if object_uid in object_cache_status.keys():\n",
    "                del object_cache_status[object_uid]  # We will log the mesh again\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841145d-a114-4693-a0d4-492f9629bbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:00<00:01, 56.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m image_stream_label \u001b[38;5;241m=\u001b[39m device_data_provider\u001b[38;5;241m.\u001b[39mget_image_stream_label(stream_id)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Retrieve the image data for a given timestamp\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m image_data \u001b[38;5;241m=\u001b[39m \u001b[43mdevice_data_provider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestamp_ns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Visualize the image data (it's a numpy array)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m log_image(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstream_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m, image\u001b[38;5;241m=\u001b[39mimage_data)\n",
      "File \u001b[0;32m~/dir/hot3d/hot3d/data_loaders/AriaDataProvider.py:110\u001b[0m, in \u001b[0;36mAriaDataProvider.get_image\u001b[0;34m(self, timestamp_ns, stream_id)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, timestamp_ns: \u001b[38;5;28mint\u001b[39m, stream_id: StreamId) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 110\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vrs_data_provider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_data_by_time_ns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestamp_ns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTimeDomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTIME_CODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTimeQueryOptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLOSEST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy_array() \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Section 3.a: Object bounding boxes\n",
    "#\n",
    "#\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "\n",
    "import matplotlib.pyplot as plt # Used to display consistent colored Bounding Boxes contours\n",
    "\n",
    "# Alias over the Object box2d data provider and Device data provider (to get image data)\n",
    "object_box2d_data_provider = hot3d_data_provider.object_box2d_data_provider\n",
    "device_data_provider = hot3d_data_provider.device_data_provider\n",
    "\n",
    "# Retrieve a distinct color mapping for object bounding box\n",
    "# by using a colormap (i.e associate a object_uid to a specific color)\n",
    "object_uids = list(object_box2d_data_provider.object_uids) # list of available object_uid used to map them to [0, 1, 2, ...] indices\n",
    "object_box2d_colors = None\n",
    "if object_box2d_data_provider is not None:\n",
    "    color_map = plt.get_cmap(\"viridis\")\n",
    "    object_box2d_colors = color_map(\n",
    "        np.linspace(0, 1, len(object_uids))\n",
    "    )\n",
    "else:\n",
    "    print(\"This section expect to have valid bounding box data\")\n",
    "\n",
    "\n",
    "# Init a rerun context\n",
    "rr.init(\"Object bounding boxed and visibility ratio\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# Use SLAM-LEFT image (exists for both Aria and Quest files)\n",
    "stream_id = StreamId(\"1201-1\")\n",
    "if stream_id not in object_box2d_data_provider.stream_ids:\n",
    "    print(f\"The object_box2d_data_provider does not have data for this StreamId: {stream_id}\")\n",
    "\n",
    "\n",
    "# Limit to the some timestamps\n",
    "for timestamp_ns in tqdm(timestamps[100:200]):\n",
    "\n",
    "    rr.set_time_nanos(\"synchronization_time\", int(timestamp_ns))\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp_ns)\n",
    "\n",
    "    # Retrieve data for this timestamp and specific stream_id\n",
    "    box2d_collection_with_dt = (\n",
    "        object_box2d_data_provider.get_bbox_at_timestamp(\n",
    "            stream_id=stream_id,\n",
    "            timestamp_ns=timestamp_ns,\n",
    "            time_query_options=TimeQueryOptions.CLOSEST,\n",
    "            time_domain=TimeDomain.TIME_CODE,\n",
    "        )\n",
    "    )\n",
    "    if box2d_collection_with_dt is None:\n",
    "        continue\n",
    "    if (\n",
    "        box2d_collection_with_dt is None\n",
    "        and box2d_collection_with_dt.box2d_collection or None\n",
    "    ):\n",
    "        continue\n",
    "    \n",
    "    # We have valid data, returned as a collection\n",
    "    # i.e for each object_uid, we retrieve its BBOX and visibility\n",
    "    object_uids_at_query_timestamp = (\n",
    "        box2d_collection_with_dt.box2d_collection.object_uid_list\n",
    "    )\n",
    "\n",
    "    for object_uid in object_uids_at_query_timestamp:\n",
    "        object_name = object_library.object_id_to_name_dict[object_uid]\n",
    "        axis_aligned_box2d = box2d_collection_with_dt.box2d_collection.box2ds[object_uid]\n",
    "        bbox = axis_aligned_box2d.box2d\n",
    "        visibility_ratio = axis_aligned_box2d.visibility_ratio\n",
    "        if bbox is None:\n",
    "            continue\n",
    "\n",
    "        rr.log(\n",
    "            f\"{stream_id}_raw/bbox/{object_name}\",\n",
    "            rr.Boxes2D(\n",
    "                mins=[bbox.left, bbox.top],\n",
    "                sizes=[bbox.width, bbox.height],\n",
    "                colors=object_box2d_colors[object_uids.index(object_uid)],\n",
    "            ),\n",
    "        )\n",
    "        rr.log(f\"visibility_ratio/{object_name}\", rr.Scalar(visibility_ratio))\n",
    "        \n",
    "        # Log the corresponding image\n",
    "        image_stream_label = device_data_provider.get_image_stream_label(stream_id)\n",
    "        # Retrieve the image data for a given timestamp\n",
    "        image_data = device_data_provider.get_image(timestamp_ns, stream_id)\n",
    "        # Visualize the image data (it's a numpy array)\n",
    "        log_image(label=f\"{stream_id}_raw\", image=image_data)\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1daf98-2df6-4d0a-ae38-331060353b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'object_box2d_colors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     72\u001b[0m rr\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstream_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_raw/bbox/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhand_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m     rr\u001b[38;5;241m.\u001b[39mBoxes2D(\n\u001b[1;32m     75\u001b[0m         mins\u001b[38;5;241m=\u001b[39m[bbox\u001b[38;5;241m.\u001b[39mleft, bbox\u001b[38;5;241m.\u001b[39mtop],\n\u001b[1;32m     76\u001b[0m         sizes\u001b[38;5;241m=\u001b[39m[bbox\u001b[38;5;241m.\u001b[39mwidth, bbox\u001b[38;5;241m.\u001b[39mheight],\n\u001b[0;32m---> 77\u001b[0m         colors\u001b[38;5;241m=\u001b[39m\u001b[43mobject_box2d_colors\u001b[49m[hand_uids\u001b[38;5;241m.\u001b[39mindex(hand_uid)],\n\u001b[1;32m     78\u001b[0m     ),\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m rr\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisibility_ratio/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhand_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, rr\u001b[38;5;241m.\u001b[39mScalar(visibility_ratio))\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Log the corresponding image\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'object_box2d_colors' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Section 3.b: Hand bounding boxes\n",
    "#\n",
    "#\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "from data_loaders.loader_hand_poses import LEFT_HAND_INDEX, RIGHT_HAND_INDEX\n",
    "import matplotlib.pyplot as plt # Used to display consistent colored Bounding Boxes contours\n",
    "\n",
    "# Alias over the Hand box2d data provider and Device data provider (to get image data)\n",
    "hand_box2d_data_provider = hot3d_data_provider.hand_box2d_data_provider\n",
    "device_data_provider = hot3d_data_provider.device_data_provider\n",
    "\n",
    "# Retrieve a distinct color mapping for hand bounding box\n",
    "# by using a colormap (i.e associate a hand_uid to a specific color)\n",
    "hand_uids = [LEFT_HAND_INDEX, RIGHT_HAND_INDEX]\n",
    "hand_box2d_colors = None\n",
    "if hand_box2d_data_provider is not None:\n",
    "    color_map = plt.get_cmap(\"viridis\")\n",
    "    hand_box2d_colors = color_map(\n",
    "        np.linspace(0, 1, len(hand_uids))\n",
    "    )\n",
    "else:\n",
    "    print(\"This section expect to have valid bounding box data\")\n",
    "\n",
    "\n",
    "# Init a rerun context\n",
    "rr.init(\"Hand bounding boxed and visibility ratio\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# Use SLAM-LEFT image (exists for both Aria and Quest files)\n",
    "stream_id = StreamId(\"1201-1\")\n",
    "if stream_id not in hand_box2d_data_provider.stream_ids:\n",
    "    print(f\"The hand_box2d_data_provider does not have data for this StreamId: {stream_id}\")\n",
    "\n",
    "\n",
    "# Limit to the some timestamps\n",
    "for timestamp_ns in tqdm(timestamps[100:200]):\n",
    "\n",
    "    rr.set_time_nanos(\"synchronization_time\", int(timestamp_ns))\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp_ns)\n",
    "\n",
    "    # Retrieve data for this timestamp and specific stream_id\n",
    "    box2d_collection_with_dt = (\n",
    "        hand_box2d_data_provider.get_bbox_at_timestamp(\n",
    "            stream_id=stream_id,\n",
    "            timestamp_ns=timestamp_ns,\n",
    "            time_query_options=TimeQueryOptions.CLOSEST,\n",
    "            time_domain=TimeDomain.TIME_CODE,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if box2d_collection_with_dt is None:\n",
    "        continue\n",
    "    if (\n",
    "        box2d_collection_with_dt is None\n",
    "        and box2d_collection_with_dt.box2d_collection or None\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # We have valid data, returned as a collection\n",
    "    # i.e for each hand_uid, we retrieve its BBOX and visibility\n",
    "    for hand_uid in hand_uids:\n",
    "        hand_name = \"left\" if hand_uid == LEFT_HAND_INDEX else \"right\"\n",
    "        axis_aligned_box2d = box2d_collection_with_dt.box2d_collection.box2ds[hand_uid]\n",
    "        bbox = axis_aligned_box2d.box2d\n",
    "        visibility_ratio = axis_aligned_box2d.visibility_ratio\n",
    "        if bbox is None:\n",
    "            continue\n",
    "\n",
    "        rr.log(\n",
    "            f\"{stream_id}_raw/bbox/{hand_name}\",\n",
    "            rr.Boxes2D(\n",
    "                mins=[bbox.left, bbox.top],\n",
    "                sizes=[bbox.width, bbox.height],\n",
    "                colors=object_box2d_colors[hand_uids.index(hand_uid)],\n",
    "            ),\n",
    "        )\n",
    "        rr.log(f\"visibility_ratio/{hand_name}\", rr.Scalar(visibility_ratio))\n",
    "        \n",
    "        # Log the corresponding image\n",
    "        image_stream_label = device_data_provider.get_image_stream_label(stream_id)\n",
    "        # Retrieve the image data for a given timestamp\n",
    "        image_data = device_data_provider.get_image(timestamp_ns, stream_id)\n",
    "        # Visualize the image data (it's a numpy array)\n",
    "        log_image(label=f\"{stream_id}_raw\", image=image_data)\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488570b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# Section 3.a: Object bounding boxes\n",
    "#\n",
    "#\n",
    "from tqdm import tqdm\n",
    "\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "\n",
    "import matplotlib.pyplot as plt # Used to display consistent colored Bounding Boxes contours\n",
    "\n",
    "# Alias over the Object box2d data provider and Device data provider (to get image data)\n",
    "object_box2d_data_provider = hot3d_data_provider.object_box2d_data_provider\n",
    "device_data_provider = hot3d_data_provider.device_data_provider\n",
    "\n",
    "# Retrieve a distinct color mapping for object bounding box\n",
    "# by using a colormap (i.e associate a object_uid to a specific color)\n",
    "object_uids = list(object_box2d_data_provider.object_uids) # list of available object_uid used to map them to [0, 1, 2, ...] indices\n",
    "object_box2d_colors = None\n",
    "if object_box2d_data_provider is not None:\n",
    "    color_map = plt.get_cmap(\"viridis\")\n",
    "    object_box2d_colors = color_map(\n",
    "        np.linspace(0, 1, len(object_uids))\n",
    "    )\n",
    "else:\n",
    "    print(\"This section expect to have valid bounding box data\")\n",
    "\n",
    "\n",
    "# Init a rerun context\n",
    "rr.init(\"Object bounding boxed and visibility ratio\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# Use SLAM-LEFT image (exists for both Aria and Quest files)\n",
    "stream_id = StreamId(\"1201-1\")\n",
    "if stream_id not in object_box2d_data_provider.stream_ids:\n",
    "    print(f\"The object_box2d_data_provider does not have data for this StreamId: {stream_id}\")\n",
    "\n",
    "\n",
    "# Limit to the some timestamps\n",
    "for timestamp_ns in tqdm(timestamps[100:200]):\n",
    "\n",
    "    rr.set_time_nanos(\"synchronization_time\", int(timestamp_ns))\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp_ns)\n",
    "\n",
    "    # Retrieve data for this timestamp and specific stream_id\n",
    "    box2d_collection_with_dt = (\n",
    "        object_box2d_data_provider.get_bbox_at_timestamp(\n",
    "            stream_id=stream_id,\n",
    "            timestamp_ns=timestamp_ns,\n",
    "            time_query_options=TimeQueryOptions.CLOSEST,\n",
    "            time_domain=TimeDomain.TIME_CODE,\n",
    "        )\n",
    "    )\n",
    "    if box2d_collection_with_dt is None:\n",
    "        continue\n",
    "    if (\n",
    "        box2d_collection_with_dt is None\n",
    "        and box2d_collection_with_dt.box2d_collection or None\n",
    "    ):\n",
    "        continue\n",
    "    \n",
    "    # We have valid data, returned as a collection\n",
    "    # i.e for each object_uid, we retrieve its BBOX and visibility\n",
    "    object_uids_at_query_timestamp = (\n",
    "        box2d_collection_with_dt.box2d_collection.object_uid_list\n",
    "    )\n",
    "\n",
    "    for object_uid in object_uids_at_query_timestamp:\n",
    "        object_name = object_library.object_id_to_name_dict[object_uid]\n",
    "        axis_aligned_box2d = box2d_collection_with_dt.box2d_collection.box2ds[object_uid]\n",
    "        bbox = axis_aligned_box2d.box2d\n",
    "        visibility_ratio = axis_aligned_box2d.visibility_ratio\n",
    "        if bbox is None:\n",
    "            continue\n",
    "\n",
    "        rr.log(\n",
    "            f\"{stream_id}_raw/bbox/{object_name}\",\n",
    "            rr.Boxes2D(\n",
    "                mins=[bbox.left, bbox.top],\n",
    "                sizes=[bbox.width, bbox.height],\n",
    "                colors=object_box2d_colors[object_uids.index(object_uid)],\n",
    "            ),\n",
    "        )\n",
    "        rr.log(f\"visibility_ratio/{object_name}\", rr.Scalar(visibility_ratio))\n",
    "        \n",
    "        # Log the corresponding image\n",
    "        image_stream_label = device_data_provider.get_image_stream_label(stream_id)\n",
    "        # Retrieve the image data for a given timestamp\n",
    "        image_data = device_data_provider.get_image(timestamp_ns, stream_id)\n",
    "        # Visualize the image data (it's a numpy array)\n",
    "        log_image(label=f\"{stream_id}_raw\", image=image_data)\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b18e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-07T12:32:08Z INFO  re_sdk_comms::server] Hosting a SDK server over TCP at 0.0.0.0:9876. Connect with the Rerun logging SDK.\n",
      "Error: winit EventLoopError: os error at /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/winit-0.29.9/src/platform_impl/linux/mod.rs:776: neither WAYLAND_DISPLAY nor DISPLAY is set. -> os error at /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/winit-0.29.9/src/platform_impl/linux/mod.rs:776: neither WAYLAND_DISPLAY nor DISPLAY is set.\n",
      "[2025-05-07T12:32:09Z WARN  re_sdk_comms::buffered_client] Failed to send message after 3 attempts: Failed to connect to Rerun server at 127.0.0.1:9876: Connection refused (os error 111)\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import rerun as rr  # @manual\n",
    "\n",
    "from data_loaders.hand_common import LANDMARK_CONNECTIVITY\n",
    "from data_loaders.headsets import Headset\n",
    "from data_loaders.loader_hand_poses import HandType\n",
    "from data_loaders.loader_object_library import ObjectLibrary\n",
    "from projectaria_tools.core.stream_id import StreamId  # @manual\n",
    "\n",
    "try:\n",
    "    from dataset_api import Hot3dDataProvider  # @manual\n",
    "except ImportError:\n",
    "    from hot3d.dataset_api import Hot3dDataProvider\n",
    "\n",
    "from data_loaders.HandDataProviderBase import (  # @manual\n",
    "    HandDataProviderBase,\n",
    "    HandPose3dCollectionWithDt,\n",
    ")\n",
    "from data_loaders.ObjectBox2dDataProvider import (  # @manual\n",
    "    ObjectBox2dCollectionWithDt,\n",
    "    ObjectBox2dProvider,\n",
    ")\n",
    "\n",
    "from data_loaders.ObjectPose3dProvider import (  # @manual\n",
    "    ObjectPose3dCollectionWithDt,\n",
    "    ObjectPose3dProvider,\n",
    ")\n",
    "\n",
    "from projectaria_tools.core.calibration import (\n",
    "    CameraCalibration,\n",
    "    DeviceCalibration,\n",
    "    FISHEYE624,\n",
    "    LINEAR,\n",
    ")\n",
    "from projectaria_tools.core.mps import get_eyegaze_point_at_depth  # @manual\n",
    "\n",
    "from projectaria_tools.core.mps.utils import (  # @manual\n",
    "    filter_points_from_confidence,\n",
    "    filter_points_from_count,\n",
    ")\n",
    "\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions  # @manual\n",
    "from projectaria_tools.core.sophus import SE3  # @manual\n",
    "from projectaria_tools.utils.rerun_helpers import (  # @manual\n",
    "    AriaGlassesOutline,\n",
    "    ToTransform3D,\n",
    ")\n",
    "\n",
    "\n",
    "class Hot3DVisualizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hot3d_data_provider,\n",
    "        hand_type: HandType = HandType.Umetrack,\n",
    "        **kargs\n",
    "    ) -> None:\n",
    "        for key, value in kargs.items():\n",
    "            setattr(self, key, value)\n",
    "        self._hot3d_data_provider = hot3d_data_provider\n",
    "        # Device calibration and Image stream data\n",
    "        self._device_data_provider = hot3d_data_provider.device_data_provider\n",
    "        # Data provider at time T (for device & objects & hand poses)\n",
    "        self._device_pose_provider = hot3d_data_provider.device_pose_data_provider\n",
    "        self._hand_data_provider = (\n",
    "            hot3d_data_provider.umetrack_hand_data_provider\n",
    "            if hand_type == HandType.Umetrack\n",
    "            else hot3d_data_provider.mano_hand_data_provider\n",
    "        )\n",
    "        if hand_type is HandType.Umetrack:\n",
    "            print(\"Hot3DVisualizer is using UMETRACK hand model\")\n",
    "        elif hand_type is HandType.Mano:\n",
    "            print(\"Hot3DVisualizer is using MANO hand model\")\n",
    "        self._object_pose_data_provider = hot3d_data_provider.object_pose_data_provider\n",
    "        self._object_box2d_data_provider = (\n",
    "            hot3d_data_provider.object_box2d_data_provider\n",
    "        )\n",
    "        # Object library\n",
    "        self._object_library = hot3d_data_provider.object_library\n",
    "\n",
    "        # If required\n",
    "        # Retrieve a distinct color mapping for object bounding box to show consistent color across stream_ids\n",
    "        # - Use a Colormap for visualizing object bounding box\n",
    "        self._object_box2d_colors = None\n",
    "        if self._object_box2d_data_provider is not None:\n",
    "            color_map = plt.get_cmap(\"viridis\")\n",
    "            self._object_box2d_colors = color_map(\n",
    "                np.linspace(0, 1, len(self._object_box2d_data_provider.object_uids))\n",
    "            )\n",
    "\n",
    "        # Keep track of what 3D assets has been loaded/unloaded so we will load them only when needed\n",
    "        self._object_cache_status = {}\n",
    "\n",
    "        # To be parametrized later\n",
    "        self._jpeg_quality = 75\n",
    "\n",
    "    def log_static_assets(\n",
    "        self,\n",
    "        image_stream_ids: List[StreamId],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log all static assets (aka Timeless assets)\n",
    "        - assets that are immutable (but can still move if attached to a 3D Pose)\n",
    "        \"\"\"\n",
    "\n",
    "        # Configure the world coordinate system to ease navigation\n",
    "        if self._hot3d_data_provider.get_device_type() is Headset.Aria:\n",
    "            rr.log(\"world\", rr.ViewCoordinates.RIGHT_HAND_Z_UP, static=True)\n",
    "        else:\n",
    "            rr.log(\"world\", rr.ViewCoordinates.RIGHT_HAND_Y_UP, static=True)\n",
    "\n",
    "        if self._hot3d_data_provider.get_device_type() is Headset.Aria:\n",
    "            ## for Aria devices, we use online calibration which is a dynamic asset\n",
    "            pass\n",
    "        elif self._hot3d_data_provider.get_device_type() is Headset.Quest3:\n",
    "            # For each of the stream ids we want to use, export the camera calibration (intrinsics and extrinsics)\n",
    "            for stream_id in image_stream_ids:\n",
    "                #\n",
    "                # Plot the camera configuration\n",
    "                [extrinsics, intrinsics] = (\n",
    "                    self._device_data_provider.get_camera_calibration(stream_id)\n",
    "                )\n",
    "                Hot3DVisualizer.log_pose(\n",
    "                    f\"world/device/{stream_id}\", extrinsics, static=True\n",
    "                )\n",
    "                Hot3DVisualizer.log_calibration(f\"world/device/{stream_id}\", intrinsics)\n",
    "\n",
    "        # Deal with Aria specifics\n",
    "        # - Glasses outline\n",
    "        # - Point cloud\n",
    "        if self._hot3d_data_provider.get_device_type() is Headset.Aria:\n",
    "            Hot3DVisualizer.log_aria_glasses(\n",
    "                \"world/device/glasses_outline\",\n",
    "                self._device_data_provider.get_device_calibration(),\n",
    "            )\n",
    "\n",
    "            # Point cloud (downsampled for visualization)\n",
    "            point_cloud = self._device_data_provider.get_point_cloud()\n",
    "            if point_cloud:\n",
    "                # Filter out low confidence points\n",
    "                threshold_invdep = 5e-4\n",
    "                threshold_dep = 5e-4\n",
    "                point_cloud = filter_points_from_confidence(\n",
    "                    point_cloud, threshold_invdep, threshold_dep\n",
    "                )\n",
    "                # Down sample points\n",
    "                points_data_down_sampled = filter_points_from_count(\n",
    "                    point_cloud, 500_000\n",
    "                )\n",
    "                # Retrieve point position\n",
    "                point_positions = [it.position_world for it in points_data_down_sampled]\n",
    "                POINT_COLOR = [200, 200, 200]\n",
    "                rr.log(\n",
    "                    \"world/points\",\n",
    "                    rr.Points3D(point_positions, colors=POINT_COLOR, radii=0.002),\n",
    "                    static=True,\n",
    "                )\n",
    "\n",
    "    def log_dynamic_assets(\n",
    "        self,\n",
    "        stream_ids: List[StreamId],\n",
    "        timestamp_ns: int,\n",
    "        idx: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log dynamic assets:\n",
    "        I.e assets that are moving, such as:\n",
    "        - 3D assets\n",
    "        - Device pose\n",
    "        - Hands\n",
    "        - Object poses\n",
    "        - Image related specifics assets\n",
    "        - images (stream_ids)\n",
    "        - Object Bounding boxes\n",
    "        - Aria Eye Gaze\n",
    "        \"\"\"\n",
    "\n",
    "        #\n",
    "        ## Retrieve and log data that is not stream_id dependent (pure 3D data)\n",
    "        #\n",
    "        acceptable_time_delta = 0\n",
    "        \n",
    "        self.time = idx\n",
    "\n",
    "        if self._hot3d_data_provider.get_device_type() is Headset.Aria:\n",
    "            # For each of the stream ids we want to use, export the camera calibration (intrinsics and extrinsics)\n",
    "            for stream_id in stream_ids:\n",
    "                #\n",
    "                # Plot the camera configuration\n",
    "                [extrinsics, intrinsics] = (\n",
    "                    self._device_data_provider.get_online_camera_calibration(\n",
    "                        stream_id=stream_id, timestamp_ns=timestamp_ns\n",
    "                    )\n",
    "                )\n",
    "                Hot3DVisualizer.log_pose(f\"world/device/{stream_id}\", extrinsics)\n",
    "                Hot3DVisualizer.log_calibration(f\"world/device/{stream_id}\", intrinsics)\n",
    "\n",
    "        elif self._hot3d_data_provider.get_device_type() is Headset.Quest3:\n",
    "            ## for Quest devices we will use factory calibration which is a static asset\n",
    "            pass\n",
    "\n",
    "        headset_pose3d_with_dt = None\n",
    "        if self._device_data_provider is not None:\n",
    "            headset_pose3d_with_dt = self._device_pose_provider.get_pose_at_timestamp(\n",
    "                timestamp_ns=timestamp_ns,\n",
    "                time_query_options=TimeQueryOptions.CLOSEST,\n",
    "                time_domain=TimeDomain.TIME_CODE,\n",
    "                acceptable_time_delta=acceptable_time_delta,\n",
    "            )\n",
    "\n",
    "        hand_poses_with_dt = None\n",
    "        if self._hand_data_provider is not None:\n",
    "            hand_poses_with_dt = self._hand_data_provider.get_pose_at_timestamp(\n",
    "                timestamp_ns=timestamp_ns,\n",
    "                time_query_options=TimeQueryOptions.CLOSEST,\n",
    "                time_domain=TimeDomain.TIME_CODE,\n",
    "                acceptable_time_delta=acceptable_time_delta,\n",
    "            )\n",
    "\n",
    "        object_poses_with_dt = None\n",
    "        if self._object_pose_data_provider is not None:\n",
    "            object_poses_with_dt = (\n",
    "                self._object_pose_data_provider.get_pose_at_timestamp(\n",
    "                    timestamp_ns=timestamp_ns,\n",
    "                    time_query_options=TimeQueryOptions.CLOSEST,\n",
    "                    time_domain=TimeDomain.TIME_CODE,\n",
    "                    acceptable_time_delta=acceptable_time_delta,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        aria_eye_gaze_data = (\n",
    "            self._device_data_provider.get_eye_gaze(timestamp_ns)\n",
    "            if self._hot3d_data_provider.get_device_type() is Headset.Aria\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        #\n",
    "        ## Log Device pose\n",
    "        #\n",
    "        if headset_pose3d_with_dt is not None:\n",
    "            headset_pose3d = headset_pose3d_with_dt.pose3d\n",
    "            Hot3DVisualizer.log_pose(\n",
    "                \"world/device\", headset_pose3d.T_world_device, static=False\n",
    "            )\n",
    "\n",
    "        #\n",
    "        ## Log Hand poses\n",
    "        #\n",
    "        Hot3DVisualizer.log_hands(\n",
    "            \"world/hands\",  # /{handedness_label}/... will be added as necessary\n",
    "            self._hand_data_provider,\n",
    "            hand_poses_with_dt,\n",
    "            show_hand_mesh=True,\n",
    "            show_hand_vertices=False,\n",
    "            show_hand_landmarks=False,\n",
    "        )\n",
    "\n",
    "        #\n",
    "        ## Log Object poses\n",
    "        #\n",
    "        Hot3DVisualizer.log_object_poses(\n",
    "            \"world/objects\",\n",
    "            object_poses_with_dt,\n",
    "            self._object_pose_data_provider,\n",
    "            self._object_library,\n",
    "            self._object_cache_status,\n",
    "        )\n",
    "\n",
    "        #\n",
    "        ## Log stream dependent data\n",
    "        #\n",
    "        for stream_id in stream_ids:\n",
    "            #\n",
    "            ## Log Image data\n",
    "            #\n",
    "            \n",
    "            #\n",
    "            ## Eye Gaze image reprojection\n",
    "            #\n",
    "            if self._hot3d_data_provider.get_device_type() is Headset.Aria:\n",
    "                # We are showing EyeGaze reprojection only on the RGB image stream\n",
    "                if stream_id != StreamId(\"214-1\"):\n",
    "                    continue\n",
    "\n",
    "                # Reproject EyeGaze for raw and pinhole images\n",
    "                camera_configurations = [FISHEYE624, LINEAR]\n",
    "                for camera_model in camera_configurations:\n",
    "                    eye_gaze_reprojection_data = (\n",
    "                        self._device_data_provider.get_eye_gaze_in_camera(\n",
    "                            stream_id, timestamp_ns, camera_model=camera_model\n",
    "                        )\n",
    "                    )\n",
    "                    if (\n",
    "                        eye_gaze_reprojection_data is None\n",
    "                        or not eye_gaze_reprojection_data.any()\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    label = (\n",
    "                        f\"world/device/{stream_id}/eye-gaze_projection\"\n",
    "                        if camera_model == LINEAR\n",
    "                        else f\"world/device/{stream_id}_raw/eye-gaze_projection_raw\"\n",
    "                    )\n",
    "                    rr.log(\n",
    "                        label,\n",
    "                        rr.Points2D(eye_gaze_reprojection_data, radii=20),\n",
    "                        # TODO consistent color and size depending of camera resolution\n",
    "                    )\n",
    "\n",
    "            # Undistorted image (required if you want see reprojected 3D mesh on the images)\n",
    "            image_data = self._device_data_provider.get_undistorted_image(\n",
    "                timestamp_ns, stream_id\n",
    "            )\n",
    "            if image_data is not None:\n",
    "                rr.log(\n",
    "                    f\"world/device/{stream_id}\",\n",
    "                    rr.Image(image_data).compress(jpeg_quality=self._jpeg_quality),\n",
    "                )\n",
    "\n",
    "            # Raw device images (required for object bounding box visualization)\n",
    "            image_data = self._device_data_provider.get_image(timestamp_ns, stream_id)\n",
    "            if image_data is not None:\n",
    "                rr.log(\n",
    "                    f\"world/device/{stream_id}_raw\",\n",
    "                    rr.Image(image_data).compress(jpeg_quality=self._jpeg_quality),\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                self._object_box2d_data_provider is not None\n",
    "                and stream_id in self._object_box2d_data_provider.stream_ids\n",
    "            ):\n",
    "                box2d_collection_with_dt = (\n",
    "                    self._object_box2d_data_provider.get_bbox_at_timestamp(\n",
    "                        stream_id=stream_id,\n",
    "                        timestamp_ns=timestamp_ns,\n",
    "                        time_query_options=TimeQueryOptions.CLOSEST,\n",
    "                        time_domain=TimeDomain.TIME_CODE,\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                if (\n",
    "                    eye_gaze_reprojection_data is None\n",
    "                    or not eye_gaze_reprojection_data.any()\n",
    "                ): \n",
    "                    print(f'gaze 없다 ~ {timestamp_ns}')\n",
    "                    continue\n",
    "                \n",
    "                self.log_object_bounding_boxes(\n",
    "                    stream_id,\n",
    "                    box2d_collection_with_dt,\n",
    "                    self._object_box2d_data_provider,\n",
    "                    self._object_library,\n",
    "                    self._object_box2d_colors,\n",
    "                    eye_gaze_reprojection_data\n",
    "                )\n",
    "\n",
    "        ## Log device dependent remaining 3D data\n",
    "        #\n",
    "\n",
    "        # Log 3D eye gaze\n",
    "        if aria_eye_gaze_data is not None:\n",
    "            T_device_CPF = self._device_data_provider.get_device_calibration().get_transform_device_cpf()\n",
    "            # Compute eye_gaze vector at depth_m (30cm for a proxy 3D vector to display)\n",
    "            gaze_vector_in_cpf = get_eyegaze_point_at_depth(\n",
    "                aria_eye_gaze_data.yaw, aria_eye_gaze_data.pitch, depth_m=0.3\n",
    "            )\n",
    "            # Draw EyeGaze vector\n",
    "            rr.log(\n",
    "                \"world/device/eye-gaze\",\n",
    "                rr.Arrows3D(\n",
    "                    origins=[T_device_CPF @ [0, 0, 0]],\n",
    "                    vectors=[\n",
    "                        T_device_CPF @ gaze_vector_in_cpf - T_device_CPF @ [0, 0, 0]\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        return self\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def log_aria_glasses(\n",
    "        label: str,\n",
    "        device_calibration: DeviceCalibration,\n",
    "        use_cad_calibration: bool = True,\n",
    "    ) -> None:\n",
    "        ## Plot Project Aria Glasses outline (as lines)\n",
    "        aria_glasses_point_outline = AriaGlassesOutline(\n",
    "            device_calibration, use_cad_calibration\n",
    "        )\n",
    "        rr.log(label, rr.LineStrips3D([aria_glasses_point_outline]), static=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def log_calibration(\n",
    "        label: str,\n",
    "        camera_calibration: CameraCalibration,\n",
    "    ) -> None:\n",
    "        rr.log(\n",
    "            label,\n",
    "            rr.Pinhole(\n",
    "                resolution=[\n",
    "                    camera_calibration.get_image_size()[0],\n",
    "                    camera_calibration.get_image_size()[1],\n",
    "                ],\n",
    "                focal_length=float(camera_calibration.get_focal_lengths()[0]),\n",
    "            ),\n",
    "            static=True,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def log_pose(label: str, pose: SE3, static=False) -> None:\n",
    "        rr.log(label, ToTransform3D(pose, False), static=static)\n",
    "\n",
    "    @staticmethod\n",
    "    def log_hands(\n",
    "        label: str,\n",
    "        hand_data_provider: HandDataProviderBase,\n",
    "        hand_poses_with_dt: HandPose3dCollectionWithDt,\n",
    "        show_hand_mesh=True,\n",
    "        show_hand_vertices=True,\n",
    "        show_hand_landmarks=True,\n",
    "    ):\n",
    "        logged_right_hand_data = False\n",
    "        logged_left_hand_data = False\n",
    "        if hand_poses_with_dt is None:\n",
    "            return\n",
    "\n",
    "        hand_pose_collection = hand_poses_with_dt.pose3d_collection\n",
    "\n",
    "        for hand_pose_data in hand_pose_collection.poses.values():\n",
    "            if hand_pose_data.is_left_hand():\n",
    "                logged_left_hand_data = True\n",
    "            elif hand_pose_data.is_right_hand():\n",
    "                logged_right_hand_data = True\n",
    "\n",
    "            handedness_label = hand_pose_data.handedness_label()\n",
    "\n",
    "            # Skeleton/Joints landmark representation\n",
    "            if show_hand_landmarks:\n",
    "                hand_landmarks = hand_data_provider.get_hand_landmarks(hand_pose_data)\n",
    "                # convert landmarks to connected lines for display\n",
    "                # (i.e retrieve points along the HAND LANDMARK_CONNECTIVITY as a list)\n",
    "                points = [\n",
    "                    connections\n",
    "                    for connectivity in LANDMARK_CONNECTIVITY\n",
    "                    for connections in [\n",
    "                        [hand_landmarks[it].numpy().tolist() for it in connectivity]\n",
    "                    ]\n",
    "                ]\n",
    "                rr.log(\n",
    "                    f\"{label}/{handedness_label}/joints\",\n",
    "                    rr.LineStrips3D(points, radii=0.002),\n",
    "                )\n",
    "\n",
    "            # Update mesh vertices if required\n",
    "            hand_mesh_vertices = (\n",
    "                hand_data_provider.get_hand_mesh_vertices(hand_pose_data)\n",
    "                if show_hand_vertices or show_hand_mesh\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Vertices representation\n",
    "            if show_hand_vertices:\n",
    "                rr.log(\n",
    "                    f\"{label}/{handedness_label}/mesh\",\n",
    "                    rr.Points3D(hand_mesh_vertices),\n",
    "                )\n",
    "\n",
    "            # Triangular Mesh representation\n",
    "            if show_hand_mesh:\n",
    "                [hand_triangles, hand_vertex_normals] = (\n",
    "                    hand_data_provider.get_hand_mesh_faces_and_normals(hand_pose_data)\n",
    "                )\n",
    "                rr.log(\n",
    "                    f\"{label}/{handedness_label}/mesh_faces\",\n",
    "                    rr.Mesh3D(\n",
    "                        vertex_positions=hand_mesh_vertices,\n",
    "                        vertex_normals=hand_vertex_normals,\n",
    "                        triangle_indices=hand_triangles,  # TODO: we could avoid sending this list if we want to save memory\n",
    "                    ),\n",
    "                )\n",
    "        # If some hand data has not been logged, do not show it in the visualizer\n",
    "        if logged_left_hand_data is False:\n",
    "            rr.log(f\"{label}/left\", rr.Clear.recursive())\n",
    "        if logged_right_hand_data is False:\n",
    "            rr.log(f\"{label}/right\", rr.Clear.recursive())\n",
    "\n",
    "    @staticmethod\n",
    "    def log_object_poses(\n",
    "        label: str,  # \"world/objects\",\n",
    "        object_poses_with_dt: ObjectPose3dCollectionWithDt,\n",
    "        object_pose_data_provider: ObjectPose3dProvider,\n",
    "        object_library: ObjectLibrary,\n",
    "        object_cache_status: Dict[int, bool],\n",
    "    ):\n",
    "        if object_poses_with_dt is None:\n",
    "            return\n",
    "\n",
    "        objects_pose3d_collection = object_poses_with_dt.pose3d_collection\n",
    "\n",
    "        # Keep a mapping to know what object has been seen, and which one has not\n",
    "        object_uids = object_pose_data_provider.object_uids_with_poses\n",
    "        logging_status = {x: False for x in object_uids}\n",
    "\n",
    "        for (\n",
    "            object_uid,\n",
    "            object_pose3d,\n",
    "        ) in objects_pose3d_collection.poses.items():\n",
    "            object_name = object_library.object_id_to_name_dict[object_uid]\n",
    "            object_name = object_name + \"_\" + str(object_uid)\n",
    "            object_cad_asset_filepath = ObjectLibrary.get_cad_asset_path(\n",
    "                object_library_folderpath=object_library.asset_folder_name,\n",
    "                object_id=object_uid,\n",
    "            )\n",
    "\n",
    "            Hot3DVisualizer.log_pose(\n",
    "                f\"world/objects/{object_name}\",\n",
    "                object_pose3d.T_world_object,\n",
    "                False,\n",
    "            )\n",
    "            # Mark object has been seen\n",
    "            logging_status[object_uid] = True\n",
    "\n",
    "            # Link the corresponding 3D object\n",
    "            if object_uid not in object_cache_status.keys():\n",
    "                object_cache_status[object_uid] = True\n",
    "                rr.log(\n",
    "                    f\"world/objects/{object_name}\",\n",
    "                    rr.Asset3D(\n",
    "                        path=object_cad_asset_filepath,\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "        # If some object are not visible, we clear the entity (last known mesh and pose will not be displayed)\n",
    "        for object_uid, displayed in logging_status.items():\n",
    "            if not displayed:\n",
    "                object_name = object_library.object_id_to_name_dict[object_uid]\n",
    "                object_name = object_name + \"_\" + str(object_uid)\n",
    "                rr.log(\n",
    "                    f\"world/objects/{object_name}\",\n",
    "                    rr.Clear.recursive(),\n",
    "                )\n",
    "                if object_uid in object_cache_status.keys():\n",
    "                    del object_cache_status[object_uid]  # We will log the mesh again\n",
    "                    \n",
    "    @staticmethod\n",
    "    def is_point_in_box(point, box):\n",
    "        \"\"\"\n",
    "        point: (x, y) 좌표\n",
    "        box: bounding box 객체 또는 dict\n",
    "            box.left, box.top, box.width, box.height가 존재해야 함\n",
    "        \"\"\"\n",
    "        x, y = point\n",
    "        x_min = box.left\n",
    "        x_max = box.left + box.width\n",
    "        y_min = box.top\n",
    "        y_max = box.top + box.height\n",
    "\n",
    "        return 1 if (x_min <= x <= x_max) and (y_min <= y <= y_max) else 0\n",
    "\n",
    "\n",
    "    def log_object_bounding_boxes(\n",
    "        self,\n",
    "        stream_id: StreamId,\n",
    "        box2d_collection_with_dt: Optional[ObjectBox2dCollectionWithDt],\n",
    "        object_box2d_data_provider: ObjectBox2dProvider,\n",
    "        object_library: ObjectLibrary,\n",
    "        bbox_colors: np.ndarray,\n",
    "        eye_gaze_reprojection_data\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Object bounding boxes (valid for native raw images).\n",
    "        - We assume that the image corresponding to the stream_id has been logged beforehand as 'world/device/{stream_id}_raw/'\n",
    "        \"\"\"\n",
    "\n",
    "        # Keep a mapping to know what object has been seen, and which one has not\n",
    "        object_uids = list(object_box2d_data_provider.object_uids)\n",
    "        logging_status = {x: False for x in object_uids}\n",
    "\n",
    "        if (\n",
    "            box2d_collection_with_dt is None\n",
    "            or box2d_collection_with_dt.box2d_collection is None\n",
    "        ):\n",
    "            # No bounding box are retrieved, we clear all the bounding box visualization existing so far\n",
    "            rr.log(f\"world/device/{stream_id}_raw/bbox\", rr.Clear.recursive())\n",
    "            return\n",
    "\n",
    "        object_uids_at_query_timestamp = (\n",
    "            box2d_collection_with_dt.box2d_collection.object_uid_list\n",
    "        )\n",
    "\n",
    "        for object_uid in object_uids_at_query_timestamp:\n",
    "            object_name = object_library.object_id_to_name_dict[object_uid]\n",
    "            axis_aligned_box2d = box2d_collection_with_dt.box2d_collection.box2ds[\n",
    "                object_uid\n",
    "            ]\n",
    "            box = axis_aligned_box2d.box2d\n",
    "            if box is None:\n",
    "                continue\n",
    "            \n",
    "            axis_aligned_box2d = box2d_collection_with_dt.box2d_collection.box2ds[object_uid]\n",
    "            visibility_ratio = axis_aligned_box2d.visibility_ratio\n",
    "            \n",
    "            logging_status[object_uid] = True\n",
    "            if visibility_ratio > 0.9: self.vis_num[object_uid] += 1 \n",
    "            \n",
    "            inside = Hot3DVisualizer.is_point_in_box(eye_gaze_reprojection_data, box)\n",
    "\n",
    "            if inside and not self.in_box[object_uid]:\n",
    "                self.enter_time[object_uid] = self.time\n",
    "                self.in_box[object_uid] = True\n",
    "\n",
    "            elif not inside and self.in_box[object_uid]:\n",
    "                duration = self.time - self.enter_time[object_uid]\n",
    "                self.durations[object_uid].append(duration)\n",
    "                self.in_box[object_uid] = False\n",
    "                self.enter_time[object_uid] = None\n",
    "            \n",
    "            rr.log(\n",
    "                f\"world/device/{stream_id}_raw/bbox/{object_name}\",\n",
    "                rr.Boxes2D(\n",
    "                    mins=[box.left, box.top],\n",
    "                    sizes=[box.width, box.height],\n",
    "                    colors=bbox_colors[object_uids.index(object_uid)],\n",
    "                ),\n",
    "            )\n",
    "        # If some object are not visible, we clear the bounding box visualization\n",
    "        for key, value in logging_status.items():\n",
    "            if not value:\n",
    "                object_name = object_library.object_id_to_name_dict[key]\n",
    "                rr.log(\n",
    "                    f\"world/device/{stream_id}_raw/bbox/{object_name}\",\n",
    "                    rr.Clear.flat(),\n",
    "                )\n",
    "rr.init(\"Hot3D\") \n",
    "\n",
    "stream_id = StreamId(\"214-1\")\n",
    "device_data_provider = hot3d_data_provider.device_data_provider\n",
    "timestamps = device_data_provider.get_sequence_timestamps()\n",
    "object_box2d_data_provider = hot3d_data_provider.object_box2d_data_provider\n",
    "object_uids = list(object_box2d_data_provider.object_uids)\n",
    "\n",
    "func_dict = {\n",
    "    'vis_num':      {uid: 0      for uid in object_uids},\n",
    "    'in_box':       {uid: False  for uid in object_uids},\n",
    "    'enter_time':   {uid: None   for uid in object_uids},\n",
    "    'durations':    {uid: []     for uid in object_uids},\n",
    "}\n",
    "gg = Hot3DVisualizer(hot3d_data_provider, HandType.Mano, **func_dict)\n",
    "\n",
    "for idx, time in enumerate(timestamps[:100]):\n",
    "    output = gg.log_dynamic_assets([stream_id], time, idx)\n",
    "output.vis_num = {output._object_library.object_id_to_name_dict.get(k, k): v for k, v in output.vis_num.items()}\n",
    "output.durations = {output._object_library.object_id_to_name_dict.get(k, k): v for k, v in output.durations.items()}\n",
    "\n",
    "print(output.vis_num)\n",
    "print(output.durations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hot3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
